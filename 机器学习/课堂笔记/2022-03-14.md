## logistic回归
在回归的基础上加了一个映射
理想情况：图像中间部分越陡越好，因为logistic是**二分类**
逻辑回归和线性回归相比，更不容易受噪声影响
![[Pasted image 20220314191623.png]]
## softmax回归
目标：**多分类**（当k=2时，就完全
退化乘成二分类的逻辑回归了）
>m：样本数量
>k：分类种类
>$1\{y_{i}=j\}$ 匹配上了就是1，没匹配上就是0？？？可能反了
>![[Pasted image 20220314185202.png]]

>$J(\theta)=-\frac{1}{m}[\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{k}1\{y^{i}=j\}log \frac{exp(\theta_{j}^{T}x^{i})}{\sum_{l=1}^{k}exp(\theta_{i}^{T}x^{i})}]$

# 第二章 模型评估
- 主要内容
	- 经验误差与过拟合 
	- 评估方法 
	- 性能度量 
	- 比较检验 
	- 偏差与方差
## 经验误差与过拟合
- 误差：
	- 定义：学习器实际预测输出与样本真实输出之间的差异
		- 数据集上的叫*训练误差*（经验误差，回忆：结构误差是啥？）
		- 测试集（训练集的补集）上的叫*泛化误差*
	- 我们追求的是*泛化误差*小的学习器
	- 模型误差包含了数据误差，或者说模型信息中包含了噪声。
- 过拟合：训练过度使泛化能力下降
## 评估方法
- 数据集由3个部分组成：
	1. 训练集
	2. 验证集
	3. 测试集（测试集应当与训练集和验证集互斥）
- 交叉验证：
	- *基本思想*：它的基本思想就是将原始数据（dataset）进行分组， 一部分做为训练集来训练模型，另一部分做为测试集来评价模型。
	- *好处*：交叉验证用于评估模型的预测性能，尤其是训练好的模 型在新数据上的表现，可以在一定程度上减小过拟合。
- 交叉验证的4种方法：
	1. `留出法 （holdout cross validation）
		- 在机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：训练集、验证集和测试集。
		- 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。
		- ![[Pasted image 20220314194437.png]]
		- 优点：操作简单
		- 缺点：
			1. 对划分的比例敏感
			2. 划分为3个集合，导致训练数据变少
	2. `k 折交叉验证（k-fold cross validation）`
		- k 折交叉验证通过对 k 个不同分组训练的结果进行平均来减少方差
		- ![[Pasted image 20220314194945.png]]
		- *步骤*：
			1. 不重复抽样将原始数据随机分为 k 份。
			2. 每一次挑选其中 1 份作为测试集，剩余 k-1 份作为训练集用于模型训练。
			3. 重复第二步 k 次，这样每个子集都有一次机会作为测试集，其余机会作为训练集。
				- 在每个训练集上训练后得到一个模型，用这个模型在相应的测试集上测试，计算并保存模型的评估指标，
			4. 计算 k 组测试结果的平均值作为模型精度的估计，并作为当前 k 折交叉验证下模型的性能指标。
		- 优点：反正全都轮一遍，怎么划分就无所谓了
		- 增强：多次 k 折交叉验证再求均值，例如：10 次 10 折交叉验证，以求更精确一点。
	3. `留一法（Leave one out cross validation）`Q^k和m是啥
		-  当 k＝m 即样本总数时，叫做留一法
			- 每次的测试集都只有一个样本，要进行 m 次训练和预测。
			- 这个方法用于训练的数据只比整体数据集少了一个样本，因此最接近 原始样本的分布。 
			- 但是训练复杂度增加了，因为模型的数量与原始数据样本数量相同。
			- 一般在数据缺乏时使用。
			-  样本数很多的话，这种方法开销很大。
	4. `自助采样法（Bootstrapping）` 
		- 在含有 m 个样本的数据集中，进行 m 次有放回地随机抽样，组成的 新数据集作为训练集。
		- 这种方法，有的样本会被多次采样，也会有一次都没有被选择过的样 本，原数据集中大概有 36.8% 的样本不会出现在新组数据集中，这 些没有被选择过的数据作为验证集。
		- ![[Pasted image 20220314201600.png]]
		- 优点：训练集的样本总数和原数据集一样都是 m，并且仍有约 1/3 的数据不被训练而可以作为测试集，对于样本数少的数据集， 就不用再由于拆分得更小而影响模型的效果。
		- 缺点：缺点是这样产生的训练集的数据分布和原数据集的不一样了，会引 入估计偏差。（毕竟由36.8%的数据一直没被用过）。因此此种方法不是很常用，除非数据量真的很少。
		- 36.8的由来：$lim_{m \rightarrow \infty}(1-\frac{1}{m})^{m}\rightarrow \frac{1}{e}\approx0.368$ 
	- 非平衡数据：我们想要的数据在数据集中非常少
		- 对于非平衡数据，可以采取分层采样，就是在每一份子集中都保持和原始数据集相同的类别比例。
## 性能度量
| 真实 | 预测         | 预测         |
| ---- | ------------ | ------------ |
|      | 正例         | 反例         |
| 正例 | TP（真正例） | FN（假反例） |
| 反例 | FP（假正例） | TN（真反例   |

* 查准率：$P=\frac{TP}{TP+FP}$
* 查全率：$R=\frac{TP}{TP+FN}$
- 两者此消彼长
- *P-R曲线*：ABC三种模型，查准率和查全率的关系
	- ![[Pasted image 20220314204942.png]]

	1. $F1$度量：
	2. $F\beta$度量
	beta>1，查全率更重要
	beta<1，查准率更重要
---
上面是针对二分类问题，如果是对于多分类问题：
- 宏查准率：
- 宏查全率：
- （对于每个分类，都由一张表，然后把所有的加权）

### ROC与AUC

### 代价敏感错误率与代价曲线
- 假判真和真判假的代价不同，如判断一个人是否得了新冠
### 偏差与方差
- 偏差度量了学习算法的偏离程度
- 方差度量了数据扰动造成的影响
- 噪声刻画了学习本身的难度