# 第三章 数据预处理于特征选择
- 主要内容：
	- 数据预处理
	- 特征选择
	- 降维
- 数据的行数和列数谁更重要呢？
	- 对于大部分规模的数据，列数更重要一点
## 3.1 数据预处理
- 数据分类
	- ![[Pasted image 20220321184306.png]]
	- 变量的种类：
		- 预测变量
		- 目标变量
	- 数据的种类：
		- 字符类型
		- 数值类型
	- 变量类型：
		- 离散型
		- 连续性
- 变量预处理的目的：
	1. 不属于同一量纲：即特征的规格不一样，不能够放在一起比较。*无量纲 化*可以解决这一问题。
	2. * 信息冗余*：对于某些定量特征，其包含的有效信息为区间划分，例如学 习成绩，假若只关心“及格”或不“及格” ，那么需要将定量的考分，转 换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。
	3. 存在*缺失值*：缺失值需要补充 
	4. 异常值检测
	5. 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的 输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定 性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常 使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将 这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特 征赋值为1，其他扩展特征赋值为0。*哑编码(独热编码)* 的方式相比直接指定的方式， 不用增加调参的工作，*对于线性模型来说，使用哑编码后的特征可达到非 线性的效果*。比如：将原来的一列“颜色”变为7列“红橙黄绿蓝靛紫”二元属性，**将一个低维上线性不可分的数据变成高维上线性可分的数据（支持向量机原理）**
	6. 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同 的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效 果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线 性的效果。
	- 使用`sklearn`中的`preproccessing`库来进行数据预处理，可以覆盖以上问题的 解决方案。
### 3.1.1 无量纲化
1. 标准化
	- $x'=\frac{x-\overline{X}}{S}$
```python
	from sklearn.preprocessing import StandardScaler
	StandardScaler().fit_transform(iris.data)
	```
1. 区间缩放法
2. 归一化
### 3.1.2 对定量特征二值化
大于阈值就职1，小于阈值就是0
### 3.1.3 对定性特征的哑编码
### 3.1.4缺失值计算
- 缺失的原因Q：
	1. 数据抽取时的缺失： 从数据库或者文件中抽取数据时候的人工失误，这是一 种比较好检测并处理的缺失值。·
	2. 数据采集阶段的缺失： 
		- *完全随机缺失*：指的是数据的缺失是随机的，数据的缺失不依赖于任何不完全变量 或完全变量。
		-  *随机缺失*：指的是数据的缺失不是完全随机的，即该类数据（年龄）缺失依赖于其他完全 变量（性别）。举一个例子，女性在年龄上的缺失值要高于男性
		-  *完全非随机缺失* ：指的是数据的缺失依赖于不完全变量自身。
- 缺失的处理方式：
	1. 删除：
		1. 删除缺失样本值（删一列）
		2. 删除存在缺失值的样本（删一行）
	2. 中位数，平均数，众数填充
		1. 直接按列填充
		2. 样本根据其他属性分类后再填充，如男性和女性的身高
	3. knn方法填充
		- 把一个样本看成一个向量，然后计算向量之间的距离，根据k个近邻进行 缺失值的填补。
		- 
### 3.1.5 样本不均衡的处理方法（假设A远多于B类）
- A类欠采样
	- 随机欠采样
	- A类分成若干子类，分别与B进入ML模型
	- 基于聚类的A类分割
- B类过采样
	- 避免欠采样造成的信息缺失
	- 自助采样，[[2022-03-14#评估方法]]
- B类数据合成
- \* 代价敏感学习
	- 降低A类权值，
	
## 3.2 特征工程
![[Pasted image 20220321193903.png]]

- 当数据预处理完成后，需要选择有意义的特征输入机器学习的算法和模型进行训练。 通常来说，从两个方面考虑来选择特征：
	1.  特征是否*发散*：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征 上基本上没有差异，这个特征对于样本的区分并没有什么用。 
	2. 特征与目标的*相关性*：这点比较显见，与目标相关性高的特征，应当优选选择。除 方差法外，其他方法均从相关性考虑。
- 根据特征选择的形式又可以将特征选择方法分为3种： 
	1. Filter：过滤法。 
		- ![[Pasted image 20220321194700.png]]
	2. Wrapper：包装法。 
	3. Embedded：嵌入法。 
	- 使用sklearn中的feature_selection库来进行特征选择。
- 方差选择法
	- 先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。 使用feature_selection库的VarianceThreshold类来选择特征的代码如下：
	- ![[Pasted image 20220321195109.png]]
- 相关系数法
	- 先要计算各个特征对目标值的相关系数以及相关系数的P值。用 feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：
	- ![[Pasted image 20220321195057.png]]
- 卡方检验
	- 卡方检验是检验定性自变量对定性因变量的相关性。但是在sklearn 中对定量的数据依然可以使用k方检验。
	- ![[Pasted image 20220321195140.png]]
- 互信息法
	- 互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式 如下：
	- ![[Pasted image 20220321195223.png]]
- 包裹式特征选择
	- 包裹式特征选择是把最终要使用的机器学习模型、评测性能的指标作为特征 选择的重要依据，每次去选择若干特征，或是排除若干特征。通常包裹式特 征选择要比过滤式的效果更好，但由于训练过程时间久，系统的开销也更大。 最典型的包裹型算法为递归特征删除算法，其原理是使用一个基模型（如： 随机森林、逻辑回归等）进行*多轮*训练（**迭代**），先用全量特征跑一个模型；删掉 5~10%的弱特征，观察准确率的变化；逐步进行，直至准确率出现大的下 滑停止。
	- ![[Pasted image 20220321195317.png]]
- 嵌入式特征选择法
	- 嵌入式特征选择法是根据机器学习的算法、模型来分析特征的重要性，从而 选择最重要的 N 个特征。与包裹式特征选择法最大的不同是，嵌入式方法是 **将特征选择过程与模型的训练过程结合为一体**，这样就可以快速地找到最佳 的特征集合，更加高效、快捷。常用的嵌入式特征选择方法有基于正则化项 （如：L1正则化，即LASSO回归）的特征选择法和基于树模型的特征选择法（如：GBDT）。 比如在电商用LR（即岭回归）做CRT预估，在3-5亿维系数的特征上用L1正则化的LR模型。 剩余2-3千万的feature，意味着其他的feature的重要程度不够。
	- ![[Pasted image 20220321195540.png]]
## 3.3 降维
### 3.3.1 主成分分析PCA（principal components analysis)
- 主成分分析（Principal components analysis，以下简称PCA）是最重要的 降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。 
- 数据集是n维的，共有m个数据(x(1),x(2),...,x(m))。希望将这m个数据的维 度从n维降到n‘维，希望这m个n’维的数据集尽可能的代表原始数据集，损失尽 可能的小。
1. 线性代数知识：
	- ![[Pasted image 20220321202213.png]]
	- ![[Pasted image 20220321202338.png]]
	- 
2. 最大可分性
	- 由上，选取不同的基对同一组数据可以给出不同的表示，若基的数量小于向量本身的维数，则达到了降维的效果
	- 一种值观的看法：希望投影后的投影值（投影值是非负的）尽可能分散。从熵的角度：熵越大，数据越桓鸾无序越好
	- 衡量分散程度的指标：
		- 方差（只能解决一维）->协方差（多维）
		- $Cov(a,b)=\frac{1}{m} \sum_{i=1}^{m}a_{i}b_{i}$
- 降维的优化目标：将一组
- 矩阵对角化：（因为Cov矩阵是一个对称矩阵，因而必然可以华为对角阵）
	- 找到k个非零特征值，就降到了k维
- 步骤：
	1. ![[Pasted image 20220321204728.png]]
- 性质：