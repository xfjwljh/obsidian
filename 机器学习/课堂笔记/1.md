# 介绍
1. 推荐书目
   1. 统计学习方法.李航.第二版

      数学推导很完整
2. 成绩
平时作业40%，课程论文60%

3. 预习
最优化：拉格朗日，
python：numpy，pandas,matplotlib,sklearn

# 线性回归
## 引言
机器学习分三类：
* 无监督学习（聚类）
* 有监督学习
  判别式的学习、创造性的学习
* 半监督学习
Q：什么是监督？

不属于以上三个类的：强化学习，属于人工智能的范畴

回归问题属于有监督学习

回归的思想：m个n维向量（x）和m个标签（y），想要得到一个函数关系，使得训练集的f(x)整体上近似等于y，然后用来预测新的的数据，得到结果

线性回归假设结果和多个特征*线性*相关

超平面：$h_\theta(x)=\theta_0 + \theta_1 x_1 +\theta_2 x_2+\ldots$一个多维的平面

$x_1\ldots x_2$是输入的数据，$\theta_1 \ldots \theta_2$是通过学习获得的参数。

不可通过学习获得的参数称为**超参**，需要人来定。如：模型中的一些参数。在经济学的角度中：外生变量，无法调节控制

要找到一个最优的拟合超平面，那么“最优”的定义是什么呢？
可以通过**损失函数**刻画，损失函数不唯一，不同的损失函数将导致不一样的结果
如将预测值和真实值的欧式距离当作损失：$J(\theta)=\frac{1}{2} \sum(H_\theta (x^i)-y(x^i))^2$
目标变成最小化损失函数，可以通过梯度下降或者最小二乘法解决

损失函数为什么要带$\frac{1}{2}$？为了求导之后比较干净。黄浩说后面会解释清楚

## 线性回归-梯度下降
思想：$\theta_{i+1} =\theta_i-\alpha \frac{\partial J(\theta)}{\partial \theta_i}\qquad i=0,1,2,\ldots,n$

$\alpha$:学习率（最优化中的步长）

问题：找到的极小点不一定是全局解

![](2022-02-28-20-47-34.png)
m:标记样本量
n：带估参数个数

* 批量梯度下降：由于在每一次迭代都考察训练集的所有样本，而称为批量梯度下降。这样计算量太大
* 随机梯度下降：单个训练样本更新θ的值，称为随机梯度下降stochastic gradient descent。比较这两种梯度下降算法，由于batch gradient descent在每一步都考虑全部数据集，因而复杂度比较高，随机梯度下降会比较快地收敛，而且在实际情况中两种梯度下降得到的最优解J(θ)一般会接近真实的最小值。所以对于较大的数据集，一般采用效率较高的随机梯度下降法。
![](2022-03-07-18-37-56.png)
全梯度下降和随机梯度下降都是极端的，折中以下mini batch ，一次用固定的100或1000个指定样本，Q 

* 最小二乘
  优美而简洁，但是机器学习中的很多问题都不是线性的，实用性较差
  最小二乘不用迭代，直接一步算出最优解
  ![](2022-03-07-18-47-12.png)
  ![](2022-03-07-18-47-21.png)
  ![](2022-03-07-19-03-16.png)

  这只是一个解析解，如果$X^TX$不可逆的话：Q这叫正则化？正则化防止过拟合。为什么加个扰动能防止过拟合？
  ![](2022-03-07-19-05-42.png)
  ![](2022-03-07-19-17-17.png)
  ![](2022-03-07-19-17-28.png)

  梯度下降像盲人摸象，无法获得全局解、最小二乘适用性不好

* 最小二乘
$y^{(i)}=\theta x^{(i)}+\epsilon^{(i)}$  
![](2022-03-07-19-21-26.png)
![](2022-03-07-19-34-16.png)
![](2022-03-07-19-34-25.png)
二分之一在这里解释了

* 局部加权线性回归
  加一个权重项$w$，越远的点权重越小
  ![](2022-03-07-19-38-28.png)

* 岭回归与Lasso
  线性回归在数据量较少的情况下会出现过拟合的现象，Ridge、Lasso可以在一定程度上解决这个问题。

  * 引入：监督学习有两大基本策略，经验风险最小化和结构风险最小化
  * 经验风险最小化策略为求解最优化问题，线性回归中的求解损失函数最小化问题即是经验风险最小化策略。但由于数据量不够，效果不好
  * 结构风险：模型本身的复杂程度
  ![](2022-03-07-20-00-55.png)